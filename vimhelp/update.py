# Regularly scheduled update: check which files need updating and process them

# TODO: use geventhttpclient instead of httpx (although this turns out to be
# painful so maybe not worth it)

# TODO: migrate to GitHub API v4, which uses GraphQL. Example query below -- use
# https://developer.github.com/v4/explorer/ to try it out (need to use the
# actual node IDs returned):
#
# {
#   repository(owner: "vim", name: "vim") {
#     refs(refPrefix: "refs/tags/", last: 1) {
#       nodes {
#         name
#       }
#     }
#     object(expression: "master:runtime/doc") {
#       ... on Tree {
#         entries {
#           type
#           name
#           oid
#           object {
#             id
#           }
#         }
#       }
#     }
#   }
#   nodes(ids: ["xyz", "abc"]) {
#     ... on Blob {
#       text
#     }
#   }
# }


import base64
import hashlib
import logging
import os
import re
from http import HTTPStatus

import flask
import flask.views
import gevent
import gevent.pool
import httpx
import werkzeug.exceptions

import google.cloud.ndb
import google.cloud.tasks

from . import secret
from .dbmodel import GlobalInfo, ProcessedFileHead, ProcessedFilePart, \
                     RawFileContent, RawFileInfo, ndb_client
from . import vimh2h

# Once we have consumed about ten minutes of CPU time, Google will throw us a
# DeadlineExceededError and our script terminates. Therefore, we must be
# careful with the order of operations, to ensure that after this has happened,
# the next scheduled run of the script can pick up where the previous one was
# interrupted. Although in practice, it takes about 30 seconds, so it's
# unlikely to be an issue.

TAGS_NAME = 'tags'
FAQ_NAME = 'vim_faq.txt'
HELP_NAME = 'help.txt'

DOC_ITEM_RE = re.compile(r'(?:[-\w]+\.txt|tags)$')
COMMIT_MSG_RE = re.compile(r'[Pp]atch\s+(\d[^:\n]+)')

URLFETCH_TIMEOUT_SECONDS = 20

GITHUB_API_URL_BASE = 'https://api.github.com'

FAQ_BASE_URL = 'https://raw.githubusercontent.com/chrisbra/vim_faq/master/doc/'

PFD_MAX_PART_LEN = 995000

# Request header name
HTTP_HDR_IF_NONE_MATCH = 'If-None-Match'

# Response header name
HTTP_HDR_ETAG = 'ETag'


class UpdateHandler(flask.views.MethodView):
    def post(self):
        # We get an HTTP POST request if the request came programmatically via
        # Cloud Tasks.
        self._run(flask.request.data)
        return flask.Response()

    def get(self):
        # We get an HTTP GET request if the request was generated by the
        # user, by entering the URL in their browser.
        self._run(flask.request.query_string)
        return "Success."

    def _run(self, request_data):
        req = flask.request

        # https://cloud.google.com/tasks/docs/creating-appengine-handlers#reading_app_engine_task_request_headers
        if 'X-AppEngine-QueueName' not in req.headers and \
                os.environ.get('VIMHELP_ENV') != 'dev' and \
                secret.UPDATE_PASSWORD not in request_data:
            raise werkzeug.exceptions.Forbidden()

        force = b'force' in request_data

        logging.info("starting %supdate", 'forced ' if force else '')

        # Use a pool to limit the number of simultaneously existing greenlets.
        # Otherwise, we may run out of memory.
        self._pool = gevent.pool.Pool(size=5)

        with ndb_client.context():
            self._g_changed = False
            self._update_g(wipe=force)
            self._do_update(no_rfi=force)

            if self._g_changed:
                self._g.put()
                logging.info("finished update, updated global info")
            else:
                logging.info("finished update, global info unchanged")

        self._pool.join()

    def _do_update(self, no_rfi):

        # Kick off retrieval of all RawFileInfo entities from the Datastore
        if no_rfi:
            all_rfi_greenlet = None
        else:
            all_rfi_greenlet = self._spawn_ndb(lambda:
                                               RawFileInfo.query().fetch())

        # Kick off check for new vim version
        refresh_vim_version_greenlet = self._spawn(self._refresh_vim_version)

        # Kick off retrieval of 'runtime/doc' dir listing in github
        docdir_greenlet = self._spawn(vim_github_request,
                                      '/repos/vim/vim/contents/runtime/doc',
                                      self._g.docdir_etag)

        # Put all RawFileInfo entites into a map
        if all_rfi_greenlet:
            rfi_map = {r.key.string_id(): r for r in all_rfi_greenlet.get()}
        else:
            rfi_map = {}

        fetcher_greenlets = set()
        fetcher_greenlets_by_name = {}

        def fetcher_greenlets_add(name, value):
            fetcher_greenlets.add(value)
            fetcher_greenlets_by_name[name] = value

        def queue_urlfetch(name, url, git_sha=None):
            rfi = rfi_map.get(name)
            etag = rfi.etag if rfi is not None else None
            logging.info("fetching %s (etag: %s)", name, etag)
            processor_greenlet = self._spawn(ProcessorHTTP.create, name,
                                             git_sha, url=url, etag=etag)
            fetcher_greenlets_add(name, processor_greenlet)

        # Kick off FAQ download

        queue_urlfetch(FAQ_NAME, FAQ_BASE_URL + FAQ_NAME)

        # Iterating over 'runtime/doc' dir listing, kick off download for all
        # modified items

        docdir = docdir_greenlet.get()

        if docdir.status_code == HTTPStatus.NOT_MODIFIED:
            logging.info("doc dir not modified")
        elif docdir.status_code == HTTPStatus.OK:
            self._g.docdir_etag = docdir.headers.get(HTTP_HDR_ETAG).encode()
            self._g_changed = True
            logging.info("got doc dir etag %s", self._g.docdir_etag)
            for item in docdir.json():
                name = item['name']
                if item['type'] == 'file' and DOC_ITEM_RE.match(name):
                    assert name not in fetcher_greenlets_by_name
                    git_sha = item['sha'].encode()
                    rfi = rfi_map.get(name)
                    if rfi is not None and rfi.git_sha == git_sha:
                        logging.debug("%s unchanged (sha=%s)", name,
                                      rfi.git_sha)
                        continue
                    elif rfi is None:
                        logging.info("%s is new (sha=%s)", name, git_sha)
                    else:
                        logging.info("%s changed (%s != %s)", name,
                                     rfi.git_sha, git_sha)
                    queue_urlfetch(name, item['download_url'], git_sha)

        # Check if we have a new vim version
        is_new_vim_version = refresh_vim_version_greenlet.get()

        # If there is no new vim version, and if the only file we're
        # downloading is the FAQ, and if the FAQ was not modified, then there
        # is nothing to do for us, so bail out now

        if not is_new_vim_version and len(fetcher_greenlets) == 1:
            faq_uf = fetcher_greenlets_by_name[FAQ_NAME].get()
            if faq_uf.http_result().status_code == HTTPStatus.NOT_MODIFIED:
                return

        def get_content(name):
            processor_greenlet = fetcher_greenlets_by_name.get(name)
            # Do we already have retrieval queued?
            if processor_greenlet is not None:
                # If so, wait for that and return the content.
                return processor_greenlet.get().raw_content()
            else:
                # If we don't have retrieval queued, that means we must already
                # have the latest version in the Datastore, so get the content
                # from there.
                return RawFileContent.get_by_id(name).data

        # Make sure we are retrieving tags, either from HTTP or from Datastore
        tags_greenlet = self._spawn_ndb(get_content, TAGS_NAME)

        # Make sure we are retrieving FAQ, either from HTTP or from Datastore
        faq_greenlet = self._spawn_ndb(get_content, FAQ_NAME)

        # If we found a new vim version and we're not already downloading
        # help.txt, kick off its retrieval from the Datastore instead
        # (since we're displaying the current vim version in the rendered
        # help.txt.html)
        if is_new_vim_version and HELP_NAME not in fetcher_greenlets_by_name:
            fetcher_greenlets_add(HELP_NAME, self._spawn_ndb(ProcessorDB.create,
                                                             HELP_NAME))

        # Construct the vimhelp-to-html converter, providing it the tags file,
        # and adding on the FAQ for extra tags
        h2h = vimh2h.VimH2H(tags_greenlet.get().decode(),
                            version=self._g.vim_version.decode())
        h2h.add_tags(FAQ_NAME, faq_greenlet.get().decode())

        # Wait for urlfetches and Datastore accesses to return; kick off the
        # processing as they do so

        logging.info("Waiting for fetchers")

        processor_greenlets = []
        for greenlet in gevent.iwait(fetcher_greenlets):
            try:
                processor = greenlet.get()
            except httpx.HTTPError as e:
                logging.error(e)
                # If we could not fetch the URL, continue with the others, but
                # set 'self._g_changed' to False so we do not save the
                # 'GlobalInfo' object at the end, so that we will retry at the
                # next run
                self._g_changed = False
            else:  # no exception was raised
                processor_greenlets.append(self._spawn_ndb(processor.process,
                                                           h2h))

        logging.info("Waiting for processors")

        gevent.joinall(processor_greenlets)

        logging.info("All done")

    def _refresh_vim_version(self):
        # Check if the Vim version has changed; we display it on our front
        # page, so we must keep it updated even if nothing else has changed

        # TODO: find a better way... this doesn't find the current vim version
        # if the latest commit did not bump the version (only a problem if we
        # don't already have the vim version in the datastore)
        #
        # should probably use the Events API:
        # https://developer.github.com/v3/activity/events/types/#pushevent
        # this is also (somewhat) compatible with webhook payloads
        # or perhaps better to use
        # https://developer.github.com/v3/repos/commits/ since that does not
        # include events we're not interested in

        # Kick off retrieval of data about latest commit on master branch,
        # which we will use to figure out if there is a new vim version
        master = vim_github_request('/repos/vim/vim/branches/master',
                                    self._g.master_etag)

        is_new_vim_version = False

        if master.status_code == HTTPStatus.OK:
            message = master.json()['commit']['commit']['message']
            m = COMMIT_MSG_RE.match(message)
            if m:
                new_vim_version = m.group(1)
                new_vim_version_b = new_vim_version.encode()
                if new_vim_version_b != self._g.vim_version:
                    logging.info("found new vim version %s (was: %s)",
                                 new_vim_version, self._g.vim_version)
                    is_new_vim_version = True
                    self._g.vim_version = new_vim_version_b
                    self._g_changed = True
                else:
                    logging.warn("master branch has moved forward, but vim "
                                 "version from commit message is unchanged: "
                                 "'%s' -> version '%s'", message,
                                 self._g.vim_version)
            else:
                logging.warn("master branch has moved forward, but no new vim "
                             "version found in commit msg ('%s'), so keeping "
                             "old one (%s)", message, self._g.vim_version)
            self._g.master_etag = master.headers.get(HTTP_HDR_ETAG).encode()
            self._g_changed = True
        elif self._g.master_etag and \
                master.status_code == HTTPStatus.NOT_MODIFIED:
            logging.info("master branch is unchanged, so no new vim version")
        else:
            logging.warn("failed to get master branch: HTTP status %d",
                         master.status_code)

        return is_new_vim_version

    def _update_g(self, wipe):
        g = GlobalInfo.get_by_id('global')

        if wipe:
            logging.info("deleting global info and raw files from db")
            greenlets = [
                self._spawn_ndb(wipe_db, RawFileContent),
                self._spawn_ndb(wipe_db, RawFileInfo)
            ]
            if g:
                greenlets.append(self._spawn_ndb(g.key.delete))
                # Make sure we preserve at least the vim version; necessary in
                # case the latest commit on master doesn't specify it
                g = GlobalInfo(id='global', vim_version=g.vim_version)
            gevent.joinall(greenlets)

        if not g:
            g = GlobalInfo(id='global')

        logging.info("global info: %s",
                     ", ".join("{} = {}".format(n, getattr(g, n)) for n in
                               g._properties.keys()))

        self._g = g

    def _spawn(self, f, *args, **kwargs):
        return self._pool.apply_async(f, args, kwargs)

    def _spawn_ndb(self, f, *args, **kwargs):
        def g():
            with ndb_client.context():
                return f(*args, **kwargs)
        return self._pool.apply_async(g)


class ProcessorHTTP:
    def __init__(self, name, git_sha, result):
        self._name = name
        self._git_sha = git_sha
        self._result = result
        self._raw_content = None

    def http_result(self):
        return self._result

    def name(self):
        return self._name

    def raw_content(self):
        if self._raw_content is None:
            r = self._result
            if r.status_code == HTTPStatus.OK:
                self._raw_content = r.content
                logging.info('ProcHTTP: got %d content bytes from server',
                             len(self._raw_content))
            elif r.status_code == HTTPStatus.NOT_MODIFIED:
                rfc = RawFileContent.get_by_id(self._name)
                self._raw_content = rfc.data
                logging.info('ProcHTTP: got %d content bytes from db',
                             len(self._raw_content))
        return self._raw_content

    def process(self, h2h):
        r = self._result
        logging.info('ProcHTTP: %s: HTTP %d', self._name, r.status_code)
        if r.status_code == HTTPStatus.OK:
            encoding = do_process(self._name, r.content, h2h)
            do_save_rawfile(self._name, self._git_sha, r.content,
                            encoding.encode(), r.headers.get(HTTP_HDR_ETAG))
        else:
            logging.info('ProcHTTP: not processing %s', self._name)

    @staticmethod
    def create(name, git_sha, **urlfetch_args):
        result = urlfetch(**urlfetch_args)
        return ProcessorHTTP(name, git_sha, result)


class ProcessorDB:
    def __init__(self, name, rfc):
        self._name = name
        self._rfc = rfc

    def name(self):
        return self._name

    def raw_content(self):
        return self._rfc.data

    def process(self, h2h):
        logging.info('ProcDB: %s: %d byte(s)', self._name,
                     len(self._rfc.data))
        do_process(self._name, self._rfc.data, h2h,
                   encoding=self._rfc.encoding.decode())

    @staticmethod
    def create(name):
        rfc = RawFileContent.get_by_id(name)
        return ProcessorDB(name, rfc)


@google.cloud.ndb.transactional(xg=True)
def save_transactional(entities):
    google.cloud.ndb.put_multi(entities)


def wipe_db(model):
    all_keys = model.query().fetch(keys_only=True)
    google.cloud.ndb.delete_multi(all_keys)


def sha1(content):
    digest = hashlib.sha1()
    digest.update(content)
    return digest.digest()


def do_process(name, content, h2h, encoding=None):
    logging.info("processing '%s' (%d byte(s))...", name, len(content))
    phead, pparts, encoding = to_html(name, content, encoding, h2h)
    logging.info("saving processed file '%s' (encoding is %s)", name, encoding)
    save_transactional([phead] + pparts)
    return encoding


def need_save_rawfilecontent(name):
    return name in (HELP_NAME, FAQ_NAME, TAGS_NAME)


def do_save_rawfile(name, git_sha, content, encoding, etag):
    rfi = RawFileInfo(id=name, git_sha=git_sha, etag=etag.encode())
    if need_save_rawfilecontent(name):
        logging.info("saving unprocessed file '%s' (info and content)", name)
        rfc = RawFileContent(id=name, data=content, encoding=encoding)
        save_transactional([rfi, rfc])
    else:
        logging.info("saving unprocessed file '%s' (info only)", name)
        rfi.put()


def to_html(name, content, encoding, h2h):
    content_str = None
    if encoding is None:
        try:
            encoding = 'UTF-8'
            content_str = content.decode(encoding)
        except UnicodeError:
            encoding = 'ISO-8859-1'
    if content_str is None:
        content_str = content.decode(encoding)
    html = h2h.to_html(name, content_str, encoding).encode()
    etag = base64.b64encode(sha1(html))
    datalen = len(html)
    phead = ProcessedFileHead(id=name, encoding=encoding.encode(), etag=etag)
    pparts = []
    if datalen > PFD_MAX_PART_LEN:
        phead.numparts = 0
        for i in range(0, datalen, PFD_MAX_PART_LEN):
            part = html[i:(i+PFD_MAX_PART_LEN)]
            if i == 0:
                phead.data0 = part
            else:
                partname = name + ':' + str(phead.numparts)
                pparts.append(ProcessedFilePart(id=partname, data=part,
                                                etag=etag))
            phead.numparts += 1
    else:
        phead.numparts = 1
        phead.data0 = html
    return phead, pparts, encoding


def vim_github_request(document, etag):
    headers = {
        'Accept':        'application/vnd.github.v3+json',
        'Authorization': 'token ' + secret.GITHUB_ACCESS_TOKEN,
    }
    return urlfetch(GITHUB_API_URL_BASE + document, etag, headers=headers)


def urlfetch(url, etag, headers=None):
    if headers is None:
        headers = {}
    if etag is not None:
        headers[HTTP_HDR_IF_NONE_MATCH] = etag
    logging.info("requesting url '%s', headers = %s", url, headers)
    result = httpx.get(url, headers=headers, timeout=URLFETCH_TIMEOUT_SECONDS)
    logging.info("response status for url %s is %s", url, result.status_code)
    return result


def handle_enqueue_update():
    req = flask.request

    is_cron = req.headers.get('X-AppEngine-Cron') == 'true'

    # https://cloud.google.com/appengine/docs/standard/python3/scheduling-jobs-with-cron-yaml?hl=en_GB#validating_cron_requests
    if not is_cron and os.environ.get('VIMHELP_ENV') != 'dev' and \
            secret.UPDATE_PASSWORD not in req.query_string:
        raise werkzeug.exceptions.Forbidden()

    logging.info("enqueueing update")

    client = google.cloud.tasks.CloudTasksClient()
    parent = client.queue_path(os.environ['GOOGLE_CLOUD_PROJECT'],
                               'us-central1', 'update2')
    task = {
        'app_engine_http_request': {
            'http_method': 'POST',
            'relative_uri': '/update',
            'body': req.query_string
        }
    }
    response = client.create_task(parent, task)
    eta = response.schedule_time.ToDatetime()
    logging.info('Task %s enqueued, ETA %s', response.name, eta)

    if is_cron:
        return flask.Response()
    else:
        return "Successfully enqueued update task."
