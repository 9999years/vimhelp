# Regularly scheduled update: check which files need updating and process them

# Ugh, the GitHub GraphQL API does not seem to support ETag:
# https://github.com/github-community/community/discussions/10799

import base64
import hashlib
import json
import logging
import os
import re
from http import HTTPStatus

import flask
import flask.views
import gevent
import gevent.pool
import werkzeug.exceptions

import google.cloud.ndb
import google.cloud.tasks

from .dbmodel import (
    GlobalInfo,
    ProcessedFileHead,
    ProcessedFilePart,
    RawFileContent,
    RawFileInfo,
    TagsInfo,
    ndb_client,
)
from .http import HttpClient, HttpError
from . import secret
from . import vimh2h

# Once we have consumed about ten minutes of CPU time, Google will throw us a
# DeadlineExceededError and our script terminates. Therefore, we must be careful with
# the order of operations, to ensure that after this has happened, the next scheduled
# run of the script can pick up where the previous one was interrupted. Although in
# practice, it takes about 30 seconds, so it's unlikely to be an issue.

# Number of concurrent (in the gevent sense) workers. Avoid setting this too high, else
# there is risk of running out of memory on our puny worker node.
CONCURRENCY = 5

TAGS_NAME = "tags"
FAQ_NAME = "vim_faq.txt"
HELP_NAME = "help.txt"

DOC_ITEM_RE = re.compile(r"(?:[-\w]+\.txt|tags)$")
VERSION_TAG_RE = re.compile(r"v?(\d[\w.+-]+)$")

GITHUB_DOWNLOAD_URL_BASE = "https://raw.githubusercontent.com/vim/vim/"
GITHUB_GRAPHQL_API_URL = "https://api.github.com/graphql"

GITHUB_GRAPHQL_QUERIES = {
    "GetRefs": """
        query GetRefs {
          repository(owner: "vim", name: "vim") {
            defaultBranchRef {
              target {
                oid
              }
            }
            refs(refPrefix: "refs/tags/",
                 orderBy: {field: TAG_COMMIT_DATE, direction: DESC},
                 first: 5) {
              nodes {
                name
              }
            }
          }
        }
        """,
    "GetDir": """
        query GetDir($expr: String) {
          repository(owner: "vim", name: "vim") {
            object(expression: $expr) {
              ... on Tree {
                entries {
                  type
                  name
                  oid
                }
              }
            }
          }
        }
        """,
}

FAQ_BASE_URL = "https://raw.githubusercontent.com/chrisbra/vim_faq/master/doc/"

PFD_MAX_PART_LEN = 995000


class UpdateHandler(flask.views.MethodView):
    def post(self):
        # We get an HTTP POST request if the request came programmatically via Cloud
        # Tasks.
        self._run(flask.request.data)
        return flask.Response()

    def get(self):
        # We get an HTTP GET request if the request was generated by the user, by
        # entering the URL in their browser.
        self._run(flask.request.query_string)
        return "Success."

    def _run(self, request_data):
        req = flask.request

        # https://cloud.google.com/tasks/docs/creating-appengine-handlers#reading_app_engine_task_request_headers
        if (
            "X-AppEngine-QueueName" not in req.headers
            and os.environ.get("VIMHELP_ENV") != "dev"
            and secret.UPDATE_PASSWORD not in request_data
        ):
            raise werkzeug.exceptions.Forbidden()

        is_force = b"force" in request_data

        logging.info("Starting %supdate", "forced " if is_force else "")

        self._http_client = HttpClient(CONCURRENCY)

        try:
            self._greenlet_pool = gevent.pool.Pool(size=CONCURRENCY)

            with ndb_client.context():
                self._g = self._init_g(wipe=is_force)
                self._g_dict_pre = self._g.to_dict()
                self._had_exception = False
                self._do_update(no_rfi=is_force)

                if not self._had_exception and self._g_dict_pre != self._g.to_dict():
                    self._g.put()
                    logging.info("Finished update, updated global info")
                else:
                    logging.info("Finished update, global info not updated")

            self._greenlet_pool.join()
        finally:
            self._http_client.close()

    def _do_update(self, no_rfi):

        old_vim_version = self._g.vim_version
        old_master_sha = self._g.master_sha

        # Kick off retrieval of some Git refs/tags
        get_git_refs_greenlet = self._spawn(self._get_git_refs)

        # Kick off retrieval of all RawFileInfo entities from the Datastore
        fetch_rfi = (lambda: []) if no_rfi else (lambda: RawFileInfo.query().fetch())
        all_rfi_greenlet = self._spawn_ndb(fetch_rfi)

        # Check whether the master branch is updated, and whether we have a new vim
        # version
        if not get_git_refs_greenlet.get():
            return
        is_master_updated = self._g.master_sha != old_master_sha
        is_new_vim_version = self._g.vim_version != old_vim_version

        if is_master_updated:
            # Kick off retrieval of 'runtime/doc' dir listing in GitHub. This is against
            # the 'master' branch, since the docs often get updated after the tagged
            # commits that introduce the relevant changes.
            docdir_expr = self._g.master_sha + ":runtime/doc"
            docdir_greenlet = self._spawn(
                self._github_graphql_request,
                "GetDir",
                variables={"expr": docdir_expr},
                etag=self._g.docdir_etag,
            )
        else:
            docdir_greenlet = None

        # Put all RawFileInfo entites into a map
        rfi_map = {r.key.string_id(): r for r in all_rfi_greenlet.get()}

        fetcher_greenlets = {}

        def queue_urlfetch(name, url, git_sha=None):
            rfi = rfi_map.get(name)
            etag = rfi.etag if rfi is not None else None
            logging.info(
                "Queueing URL fetch for '%s' (etag=%s git_sha=%s)", name, etag, git_sha
            )
            fetcher_greenlets[name] = self._spawn(
                ProcessorHTTP.create,
                name,
                git_sha,
                http_client=self._http_client,
                url=url,
                etag=etag,
            )

        # Kick off FAQ download

        queue_urlfetch(FAQ_NAME, FAQ_BASE_URL + FAQ_NAME)

        # Iterating over 'runtime/doc' dir listing, kick off download for all modified
        # items

        if docdir_greenlet is not None:
            docdir = docdir_greenlet.get()

        if docdir_greenlet is None:
            logging.info("No need to get new doc dir listing")
        elif docdir.status_code == HTTPStatus.NOT_MODIFIED:
            logging.info("Doc dir not modified")
        elif docdir.status_code == HTTPStatus.OK:
            etag = docdir.header("ETag")
            self._g.docdir_etag = etag.encode() if etag is not None else None
            logging.info("Doc dir modified, new etag is %s", docdir.header("ETag"))
            resp = json.loads(docdir.body)["data"]
            for item in resp["repository"]["object"]["entries"]:
                name = item["name"]
                if item["type"] != "blob" or not DOC_ITEM_RE.match(name):
                    continue
                assert name not in fetcher_greenlets
                git_sha = item["oid"].encode()
                rfi = rfi_map.get(name)
                if rfi is not None and rfi.git_sha == git_sha:
                    logging.debug("Found unchanged '%s'", name)
                    continue
                elif rfi is None:
                    logging.info("Found new '%s'", name)
                else:
                    logging.info("Found changed '%s'", name)
                download_url = (
                    f"{GITHUB_DOWNLOAD_URL_BASE}{self._g.master_sha}/runtime/doc/{name}"
                )
                queue_urlfetch(name, download_url, git_sha)
        else:
            logging.warning("Bad doc dir HTTP status %d; aborting", docdir.status_code)
            return

        # If there is no new vim version, and if the only file we're downloading is the
        # FAQ, and if the FAQ was not modified, then there is nothing to do for us, so
        # bail out now

        if not is_new_vim_version and len(fetcher_greenlets) == 1:
            faq_processor = fetcher_greenlets[FAQ_NAME].get()
            if faq_processor.status_code() == HTTPStatus.NOT_MODIFIED:
                logging.info("Nothing to do")
                return

        def get_content(name):
            processor_greenlet = fetcher_greenlets.get(name)
            # Do we already have retrieval queued?
            if processor_greenlet is not None:
                # If so, wait for that and return the content.
                logging.info("Getting '%s'", name)
                return processor_greenlet.get().raw_content()
            else:
                logging.info("Getting '%s' from Datastore", name)
                # If we don't have retrieval queued, that means we must already have the
                # latest version in the Datastore, so get the content from there.
                return RawFileContent.get_by_id(name).data

        # Make sure we are retrieving tags and FAQ, either via HTTP or from Datastore
        tags_greenlet = self._spawn_ndb(get_content, TAGS_NAME)
        faq_greenlet = self._spawn_ndb(get_content, FAQ_NAME)

        # If we found a new vim version and we're not already downloading help.txt, kick
        # off its retrieval from the Datastore instead (since we're displaying the
        # current vim version in the rendered help.txt.html)
        if is_new_vim_version and HELP_NAME not in fetcher_greenlets:
            fetcher_greenlets[HELP_NAME] = self._spawn_ndb(
                ProcessorDB.create, HELP_NAME
            )

        tags_body = tags_greenlet.get()

        # Construct the vimhelp-to-html converter, providing it the tags file content,
        # and adding on the FAQ for extra tags
        h2h = vimh2h.VimH2H(tags_body.decode(), version=self._g.vim_version)
        logging.info("Adding FAQ tags")
        h2h.add_tags(FAQ_NAME, faq_greenlet.get().decode())

        processor_greenlets = [self._spawn_ndb(save_tags_json, h2h)]

        # Wait for HTTP fetches and Datastore accesses to return; kick off the
        # processing as they do so

        logging.info("Waiting for fetchers")

        for greenlet in gevent.iwait(fetcher_greenlets.values()):
            try:
                processor = greenlet.get()
            except HttpError as e:
                logging.error(e)
                # If we could not fetch the URL, continue with the others, but set
                # 'self._had_exception' to True so we do not save the 'GlobalInfo'
                # object at the end, so that we will retry at the next run
                self._had_exception = True
            else:  # no exception was raised
                processor_greenlets.append(self._spawn_ndb(processor.process, h2h))

        logging.info("Waiting for processors")

        gevent.joinall(processor_greenlets)

        logging.info("All done")

    def _get_git_refs(self):
        r = self._github_graphql_request("GetRefs", etag=self._g.refs_etag)
        if r.status_code == HTTPStatus.OK:
            etag_str = r.header("ETag")
            etag = etag_str.encode() if etag_str is not None else None
            if etag == self._g.refs_etag:
                logging.info("GetRefs query ETag unchanged (%s)", etag)
            else:
                logging.info(
                    "GetRefs query ETag changed: %s -> %s", self._g.refs_etag, etag
                )
                self._g.refs_etag = etag
            resp = json.loads(r.body)["data"]["repository"]
            latest_sha = resp["defaultBranchRef"]["target"]["oid"]
            if latest_sha == self._g.master_sha:
                logging.info("master SHA unchanged (%s)", latest_sha)
            else:
                logging.info(
                    "master SHA changed: %s -> %s", self._g.master_sha, latest_sha
                )
                self._g.master_sha = latest_sha
            tags = resp["refs"]["nodes"]
            latest_version = None
            for tag in tags:
                if m := VERSION_TAG_RE.match(tag["name"]):
                    latest_version = m.group(1)
                    break
            if latest_version == self._g.vim_version:
                logging.info("Vim version unchanged (%s)", latest_version)
            else:
                logging.info(
                    "Vim version changed: %s -> %s", self._g.vim_version, latest_version
                )
                self._g.vim_version = latest_version
            return True
        elif r.status_code == HTTPStatus.NOT_MODIFIED and self._g.refs_etag:
            logging.info("Initial GraphQL request: HTTP Not Modified")
            return True
        else:
            logging.warning(
                "Initial GraphQL request: bad HTTP status %d", r.status_code
            )
            return False

    def _init_g(self, wipe):
        g = GlobalInfo.get_by_id("global")

        if wipe:
            logging.info("Deleting global info and raw files from Datastore")
            greenlets = [
                self._spawn_ndb(wipe_db, RawFileContent),
                self._spawn_ndb(wipe_db, RawFileInfo),
            ]
            if g:
                greenlets.append(self._spawn_ndb(g.key.delete))
                g = None
            gevent.joinall(greenlets)

        if not g:
            g = GlobalInfo(id="global")

        logging.info(
            "Global info: %s",
            ", ".join("{} = {}".format(n, getattr(g, n)) for n in g._properties.keys()),
        )

        return g

    def _github_graphql_request(self, query_name, variables=None, etag=None):
        logging.info("Making GitHub GraphQL query: %s", query_name)
        headers = {
            "Authorization": "token " + secret.GITHUB_ACCESS_TOKEN,
        }
        if etag is not None:
            headers["If-None-Match"] = etag.decode()
        body = {"query": GITHUB_GRAPHQL_QUERIES[query_name]}
        if variables is not None:
            body["variables"] = variables
        response = self._http_client.post(
            GITHUB_GRAPHQL_API_URL, json=body, headers=headers
        )
        logging.info("GitHub %s HTTP status: %s", query_name, response.status_code)
        return response

    def _spawn(self, f, *args, **kwargs):
        return self._greenlet_pool.apply_async(f, args, kwargs)

    def _spawn_ndb(self, f, *args, **kwargs):
        def g():
            with ndb_client.context():
                return f(*args, **kwargs)

        return self._greenlet_pool.apply_async(g)


class ProcessorHTTP:
    def __init__(self, name, git_sha, result):
        self._name = name
        self._git_sha = git_sha
        self._result = result
        self._raw_content = None

    def status_code(self):
        return self._result.status_code

    def raw_content(self):
        if self._raw_content is None:
            r = self._result
            if r.status_code == HTTPStatus.OK:
                self._raw_content = r.body
                logging.info(
                    "Got '%s' from HTTP (%d bytes)", self._name, len(self._raw_content)
                )
            elif r.status_code == HTTPStatus.NOT_MODIFIED:
                rfc = RawFileContent.get_by_id(self._name)
                self._raw_content = rfc.data
                logging.info(
                    "Got '%s' from Datastore (%d bytes)",
                    self._name,
                    len(self._raw_content),
                )
        return self._raw_content

    def process(self, h2h):
        r = self._result
        if r.status_code == HTTPStatus.OK:
            encoding = do_process(self._name, self.raw_content(), h2h)
            do_save_rawfile(
                self._name,
                self._git_sha,
                self.raw_content(),
                encoding.encode(),
                r.header("ETag"),
            )
        else:
            logging.info(
                "Not translating '%s' due to HTTP status %d", self._name, r.status_code
            )

    @staticmethod
    def create(name, git_sha, http_client, url, etag):
        headers = {}
        if etag is not None:
            headers["If-None-Match"] = etag.decode()
        logging.info("Fetching %s", url)
        response = http_client.get(url, headers)
        logging.info("Fetched %s -> HTTP %s", url, response.status_code)
        return ProcessorHTTP(name, git_sha, response)


class ProcessorDB:
    def __init__(self, name, rfc):
        logging.info("'%s': got %d bytes from Datastore", name, len(rfc.data))
        self._name = name
        self._rfc = rfc

    def raw_content(self):
        return self._rfc.data

    def process(self, h2h):
        do_process(
            self._name, self._rfc.data, h2h, encoding=self._rfc.encoding.decode()
        )

    @staticmethod
    def create(name):
        rfc = RawFileContent.get_by_id(name)
        return ProcessorDB(name, rfc)


@google.cloud.ndb.transactional(xg=True)
def save_transactional(entities):
    google.cloud.ndb.put_multi(entities)


def wipe_db(model):
    all_keys = model.query().fetch(keys_only=True)
    google.cloud.ndb.delete_multi(all_keys)


def sha1(content):
    digest = hashlib.sha1()
    digest.update(content)
    return digest.digest()


def do_process(name, content, h2h, encoding=None):
    logging.info("Translating '%s' to HTML", name)
    phead, pparts, encoding = to_html(name, content, encoding, h2h)
    logging.info(
        "Saving HTML translation of '%s' (encoded as %s) to Datastore",
        name,
        encoding,
    )
    save_transactional([phead] + pparts)
    return encoding


def do_save_rawfile(name, git_sha, content, encoding, etag):
    rfi = RawFileInfo(id=name, git_sha=git_sha, etag=etag.encode())
    if name in (HELP_NAME, FAQ_NAME, TAGS_NAME):
        logging.info("Saving raw file '%s' (info and content) to Datastore", name)
        rfc = RawFileContent(id=name, data=content, encoding=encoding)
        save_transactional([rfi, rfc])
    else:
        logging.info("Saving raw file '%s' (info only) to Datastore", name)
        rfi.put()


def save_tags_json(h2h):
    tags = h2h.sorted_tag_href_pairs()
    logging.info("Saving %d tag, href pairs", len(tags))
    TagsInfo(id="tags", tags=tags).put()


def to_html(name, content, encoding, h2h):
    content_str = None
    if encoding is None:
        try:
            encoding = "UTF-8"
            content_str = content.decode(encoding)
        except UnicodeError:
            encoding = "ISO-8859-1"
    if content_str is None:
        content_str = content.decode(encoding)
    html = h2h.to_html(name, content_str, encoding).encode()
    etag = base64.b64encode(sha1(html))
    datalen = len(html)
    phead = ProcessedFileHead(id=name, encoding=encoding.encode(), etag=etag)
    pparts = []
    if datalen > PFD_MAX_PART_LEN:
        phead.numparts = 0
        for i in range(0, datalen, PFD_MAX_PART_LEN):
            part = html[i : (i + PFD_MAX_PART_LEN)]
            if i == 0:
                phead.data0 = part
            else:
                partname = name + ":" + str(phead.numparts)
                pparts.append(ProcessedFilePart(id=partname, data=part, etag=etag))
            phead.numparts += 1
    else:
        phead.numparts = 1
        phead.data0 = html
    return phead, pparts, encoding


def handle_enqueue_update():
    req = flask.request

    is_cron = req.headers.get("X-AppEngine-Cron") == "true"

    # https://cloud.google.com/appengine/docs/standard/python3/scheduling-jobs-with-cron-yaml?hl=en_GB#validating_cron_requests
    if (
        not is_cron
        and os.environ.get("VIMHELP_ENV") != "dev"
        and secret.UPDATE_PASSWORD not in req.query_string
    ):
        raise werkzeug.exceptions.Forbidden()

    logging.info("Enqueueing update")

    client = google.cloud.tasks.CloudTasksClient()
    queue_name = client.queue_path(
        os.environ["GOOGLE_CLOUD_PROJECT"], "us-central1", "update2"
    )
    task = {
        "app_engine_http_request": {
            "http_method": "POST",
            "relative_uri": "/update",
            "body": req.query_string,
        }
    }
    response = client.create_task(parent=queue_name, task=task)
    logging.info("Task %s enqueued, ETA %s", response.name, response.schedule_time)

    if is_cron:
        return flask.Response()
    else:
        return "Successfully enqueued update task."
