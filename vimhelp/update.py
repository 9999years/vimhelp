# Regularly scheduled update: check which files need updating and process them

# TODO: migrate to GitHub API v4, which uses GraphQL. Example query below -- use
# https://developer.github.com/v4/explorer/ to try it out (need to use the
# actual node IDs returned):
#
# {
#   repository(owner: "vim", name: "vim") {
#     refs(refPrefix: "refs/tags/", last: 1) {
#       nodes {
#         name
#       }
#     }
#     object(expression: "master:runtime/doc") {
#       ... on Tree {
#         entries {
#           type
#           name
#           oid
#           object {
#             id
#           }
#         }
#       }
#     }
#   }
#   nodes(ids: ["xyz", "abc"]) {
#     ... on Blob {
#       text
#     }
#   }
# }


import base64
import hashlib
import json
import logging
import os
import re
from http import HTTPStatus

import flask
import flask.views
import gevent
import gevent.pool
import gevent.ssl
import geventhttpclient
import geventhttpclient.client
import geventhttpclient.response
import werkzeug.exceptions

import google.cloud.ndb
import google.cloud.tasks

from . import secret
from .dbmodel import GlobalInfo, ProcessedFileHead, ProcessedFilePart, \
                     RawFileContent, RawFileInfo, TagsInfo, ndb_client
from . import vimh2h

# Once we have consumed about ten minutes of CPU time, Google will throw us a
# DeadlineExceededError and our script terminates. Therefore, we must be
# careful with the order of operations, to ensure that after this has happened,
# the next scheduled run of the script can pick up where the previous one was
# interrupted. Although in practice, it takes about 30 seconds, so it's
# unlikely to be an issue.

# Number of concurrent (in the gevent sense) workers. Avoid setting this too
# high, else there is risk of running out of memory on our puny worker node.
CONCURRENCY = 5

TAGS_NAME = 'tags'
FAQ_NAME = 'vim_faq.txt'
HELP_NAME = 'help.txt'

DOC_ITEM_RE = re.compile(r'(?:[-\w]+\.txt|tags)$')
COMMIT_MSG_RE = re.compile(r'[Pp]atch\s+(\d[^:\n]+)')

GITHUB_API_URL_BASE = 'https://api.github.com'

FAQ_BASE_URL = 'https://raw.githubusercontent.com/chrisbra/vim_faq/master/doc/'

PFD_MAX_PART_LEN = 995000

# Request header name
HTTP_HDR_IF_NONE_MATCH = 'If-None-Match'

# Response header name
HTTP_HDR_ETAG = 'ETag'


class UpdateHandler(flask.views.MethodView):
    def post(self):
        # We get an HTTP POST request if the request came programmatically via
        # Cloud Tasks.
        self._run(flask.request.data)
        return flask.Response()

    def get(self):
        # We get an HTTP GET request if the request was generated by the
        # user, by entering the URL in their browser.
        self._run(flask.request.query_string)
        return "Success."

    def _run(self, request_data):
        req = flask.request

        # https://cloud.google.com/tasks/docs/creating-appengine-handlers#reading_app_engine_task_request_headers
        if 'X-AppEngine-QueueName' not in req.headers and \
                os.environ.get('VIMHELP_ENV') != 'dev' and \
                secret.UPDATE_PASSWORD not in request_data:
            raise werkzeug.exceptions.Forbidden()

        force = b'force' in request_data

        logging.info("Starting %supdate", 'forced ' if force else '')

        self._http_client_pool = geventhttpclient.client.HTTPClientPool(
            ssl_context_factory=gevent.ssl.create_default_context,
            concurrency=CONCURRENCY)

        try:
            self._greenlet_pool = gevent.pool.Pool(size=CONCURRENCY)

            with ndb_client.context():
                self._g_changed = False
                self._update_g(wipe=force)
                self._do_update(no_rfi=force)

                if self._g_changed:
                    self._g.put()
                    logging.info("Finished update, updated global info")
                else:
                    logging.info("Finished update, global info unchanged")

            self._greenlet_pool.join()
        finally:
            self._http_client_pool.close()

    def _do_update(self, no_rfi):

        # Kick off retrieval of all RawFileInfo entities from the Datastore
        if no_rfi:
            all_rfi_greenlet = None
        else:
            all_rfi_greenlet = self._spawn_ndb(lambda:
                                               RawFileInfo.query().fetch())

        # Kick off check for new vim version
        refresh_vim_version_greenlet = self._spawn(self._refresh_vim_version)

        # Kick off retrieval of 'runtime/doc' dir listing in github
        docdir_greenlet = self._spawn(self._vim_github_request,
                                      '/repos/vim/vim/contents/runtime/doc',
                                      self._g.docdir_etag)

        # Put all RawFileInfo entites into a map
        if all_rfi_greenlet is not None:
            rfi_map = {r.key.string_id(): r for r in all_rfi_greenlet.get()}
        else:
            rfi_map = {}

        fetcher_greenlets = set()
        fetcher_greenlets_by_name = {}

        def fetcher_greenlets_add(name, value):
            fetcher_greenlets.add(value)
            fetcher_greenlets_by_name[name] = value

        def queue_urlfetch(name, url, git_sha=None):
            rfi = rfi_map.get(name)
            etag = rfi.etag if rfi is not None else None
            logging.info("Queueing URL fetch for '%s' (etag: %s)", name, etag)
            processor_greenlet = self._spawn(ProcessorHTTP.create, name,
                                             git_sha,
                                             client_pool=self._http_client_pool,
                                             url=url, etag=etag)
            fetcher_greenlets_add(name, processor_greenlet)

        # Kick off FAQ download

        queue_urlfetch(FAQ_NAME, FAQ_BASE_URL + FAQ_NAME)

        # Iterating over 'runtime/doc' dir listing, kick off download for all
        # modified items

        docdir = docdir_greenlet.get()

        if docdir.status_code == HTTPStatus.NOT_MODIFIED:
            logging.info("doc dir not modified")
        elif docdir.status_code == HTTPStatus.OK:
            self._g.docdir_etag = docdir.header(HTTP_HDR_ETAG).encode()
            self._g_changed = True
            logging.info("doc dir modified, new etag is %s",
                         docdir.header(HTTP_HDR_ETAG))
            for item in json.loads(docdir.body):
                name = item['name']
                if item['type'] == 'file' and DOC_ITEM_RE.match(name):
                    assert name not in fetcher_greenlets_by_name
                    git_sha = item['sha'].encode()
                    rfi = rfi_map.get(name)
                    if rfi is not None and rfi.git_sha == git_sha:
                        logging.debug("Found unchanged '%s'", name)
                        continue
                    elif rfi is None:
                        logging.info("Found new '%s'", name)
                    else:
                        logging.info("Found changed '%s'", name)
                    queue_urlfetch(name, item['download_url'], git_sha)

        # Check if we have a new vim version
        is_new_vim_version = refresh_vim_version_greenlet.get()

        # If there is no new vim version, and if the only file we're
        # downloading is the FAQ, and if the FAQ was not modified, then there
        # is nothing to do for us, so bail out now

        if not is_new_vim_version and len(fetcher_greenlets) == 1:
            faq_uf = fetcher_greenlets_by_name[FAQ_NAME].get()
            if faq_uf.status_code() == HTTPStatus.NOT_MODIFIED:
                return

        def get_content(name):
            processor_greenlet = fetcher_greenlets_by_name.get(name)
            # Do we already have retrieval queued?
            if processor_greenlet is not None:
                # If so, wait for that and return the content.
                logging.info("Getting '%s'", name)
                return processor_greenlet.get().raw_content()
            else:
                logging.info("Getting '%s' from Datastore", name)
                # If we don't have retrieval queued, that means we must already
                # have the latest version in the Datastore, so get the content
                # from there.
                return RawFileContent.get_by_id(name).data

        # Make sure we are retrieving tags, either from HTTP or from Datastore
        tags_greenlet = self._spawn_ndb(get_content, TAGS_NAME)

        # Make sure we are retrieving FAQ, either from HTTP or from Datastore
        faq_greenlet = self._spawn_ndb(get_content, FAQ_NAME)

        # If we found a new vim version and we're not already downloading
        # help.txt, kick off its retrieval from the Datastore instead
        # (since we're displaying the current vim version in the rendered
        # help.txt.html)
        if is_new_vim_version and HELP_NAME not in fetcher_greenlets_by_name:
            fetcher_greenlets_add(HELP_NAME, self._spawn_ndb(ProcessorDB.create,
                                                             HELP_NAME))

        tags_body = tags_greenlet.get()

        # Construct the vimhelp-to-html converter, providing it the tags file,
        # and adding on the FAQ for extra tags
        h2h = vimh2h.VimH2H(tags_body.decode(),
                            version=self._g.vim_version.decode())
        logging.info("Adding FAQ tags")
        h2h.add_tags(FAQ_NAME, faq_greenlet.get().decode())

        processor_greenlets = [self._spawn_ndb(save_tags_json, h2h)]

        # Wait for urlfetches and Datastore accesses to return; kick off the
        # processing as they do so

        logging.info("Waiting for fetchers")

        for greenlet in gevent.iwait(fetcher_greenlets):
            try:
                processor = greenlet.get()
            except UrlfetchError as e:
                logging.error(e)
                # If we could not fetch the URL, continue with the others, but
                # set 'self._g_changed' to False so we do not save the
                # 'GlobalInfo' object at the end, so that we will retry at the
                # next run
                self._g_changed = False
            else:  # no exception was raised
                processor_greenlets.append(self._spawn_ndb(processor.process,
                                                           h2h))

        logging.info("Waiting for processors")

        gevent.joinall(processor_greenlets)

        logging.info("All done")

    def _refresh_vim_version(self):
        # Check if the Vim version has changed; we display it on our front
        # page, so we must keep it updated even if nothing else has changed

        # TODO: find a better way... this doesn't find the current vim version
        # if the latest commit did not bump the version (only a problem if we
        # don't already have the vim version in the datastore)
        #
        # should probably use the Events API:
        # https://developer.github.com/v3/activity/events/types/#pushevent
        # this is also (somewhat) compatible with webhook payloads
        # or perhaps better to use
        # https://developer.github.com/v3/repos/commits/ since that does not
        # include events we're not interested in

        is_new_vim_version = False

        # Kick off retrieval of data about latest commit on master branch,
        # which we will use to figure out if there is a new vim version
        master = self._vim_github_request('/repos/vim/vim/branches/master',
                                          self._g.master_etag)
        if master.status_code == HTTPStatus.OK:
            message = json.loads(master.body)['commit']['commit']['message']
            m = COMMIT_MSG_RE.match(message)
            if m:
                new_vim_version = m.group(1)
                new_vim_version_b = new_vim_version.encode()
                if new_vim_version_b != self._g.vim_version:
                    logging.info("Found new vim version '%s' (was: '%s')",
                                 new_vim_version, self._g.vim_version)
                    is_new_vim_version = True
                    self._g.vim_version = new_vim_version_b
                    self._g_changed = True
                else:
                    logging.warn("master branch has moved forward, but vim "
                                 "version from commit message is unchanged: "
                                 "'%s' -> version '%s'", message,
                                 self._g.vim_version)
            else:
                logging.warn("master branch has moved forward, but no new vim "
                             "version found in commit msg ('%s'), so keeping "
                             "old one ('%s')", message, self._g.vim_version)
            self._g.master_etag = master.header(HTTP_HDR_ETAG).encode()
            self._g_changed = True
        elif self._g.master_etag and \
                master.status_code == HTTPStatus.NOT_MODIFIED:
            logging.info("master branch is unchanged, so no new vim version")
        else:
            logging.warn("Failed to get master branch: HTTP status %d",
                         master.status_code)

        return is_new_vim_version

    def _update_g(self, wipe):
        g = GlobalInfo.get_by_id('global')

        if wipe:
            logging.info("Deleting global info and raw files from Datastore")
            greenlets = [
                self._spawn_ndb(wipe_db, RawFileContent),
                self._spawn_ndb(wipe_db, RawFileInfo)
            ]
            if g:
                greenlets.append(self._spawn_ndb(g.key.delete))
                # Make sure we preserve at least the vim version; necessary in
                # case the latest commit on master doesn't specify it
                g = GlobalInfo(id='global', vim_version=g.vim_version)
            gevent.joinall(greenlets)

        if not g:
            g = GlobalInfo(id='global')

        logging.info("Global info: %s",
                     ", ".join("{} = {}".format(n, getattr(g, n)) for n in
                               g._properties.keys()))

        self._g = g

    def _vim_github_request(self, document, etag):
        headers = {
            'Accept':        'application/vnd.github.v3+json',
            'Authorization': 'token ' + secret.GITHUB_ACCESS_TOKEN,
        }
        return urlfetch(self._http_client_pool, GITHUB_API_URL_BASE + document,
                        etag, headers=headers)

    def _spawn(self, f, *args, **kwargs):
        return self._greenlet_pool.apply_async(f, args, kwargs)

    def _spawn_ndb(self, f, *args, **kwargs):
        def g():
            with ndb_client.context():
                return f(*args, **kwargs)
        return self._greenlet_pool.apply_async(g)


class ProcessorHTTP:
    def __init__(self, name, git_sha, result):
        self._name = name
        self._git_sha = git_sha
        self._result = result
        self._raw_content = None

    def status_code(self):
        return self._result.status_code

    def name(self):
        return self._name

    def raw_content(self):
        if self._raw_content is None:
            r = self._result
            if r.status_code == HTTPStatus.OK:
                self._raw_content = r.body
                logging.info("Got '%s' from HTTP (%d bytes)",
                             self._name, len(self._raw_content))
            elif r.status_code == HTTPStatus.NOT_MODIFIED:
                rfc = RawFileContent.get_by_id(self._name)
                self._raw_content = rfc.data
                logging.info("Got '%s' from Datastore (%d bytes)",
                             self._name, len(self._raw_content))
        return self._raw_content

    def process(self, h2h):
        r = self._result
        if r.status_code == HTTPStatus.OK:
            encoding = do_process(self._name, self.raw_content(), h2h)
            do_save_rawfile(self._name, self._git_sha, self.raw_content(),
                            encoding.encode(), r.header(HTTP_HDR_ETAG))

    @staticmethod
    def create(name, git_sha, **urlfetch_args):
        result = urlfetch(**urlfetch_args)
        return ProcessorHTTP(name, git_sha, result)


class ProcessorDB:
    def __init__(self, name, rfc):
        logging.info("'%s': got %d bytes from Datastore", name, len(rfc.data))
        self._name = name
        self._rfc = rfc

    def name(self):
        return self._name

    def raw_content(self):
        return self._rfc.data

    def process(self, h2h):
        do_process(self._name, self._rfc.data, h2h,
                   encoding=self._rfc.encoding.decode())

    @staticmethod
    def create(name):
        rfc = RawFileContent.get_by_id(name)
        return ProcessorDB(name, rfc)


@google.cloud.ndb.transactional(xg=True)
def save_transactional(entities):
    google.cloud.ndb.put_multi(entities)


def wipe_db(model):
    all_keys = model.query().fetch(keys_only=True)
    google.cloud.ndb.delete_multi(all_keys)


def sha1(content):
    digest = hashlib.sha1()
    digest.update(content)
    return digest.digest()


def do_process(name, content, h2h, encoding=None):
    logging.info("Translating '%s' to HTML", name)
    phead, pparts, encoding = to_html(name, content, encoding, h2h)
    logging.info("Saving HTML translation of '%s' (encoded as %s) to "
                 "Datastore", name, encoding)
    save_transactional([phead] + pparts)
    return encoding


def need_save_rawfilecontent(name):
    return name in (HELP_NAME, FAQ_NAME, TAGS_NAME)


def do_save_rawfile(name, git_sha, content, encoding, etag):
    rfi = RawFileInfo(id=name, git_sha=git_sha, etag=etag.encode())
    if need_save_rawfilecontent(name):
        logging.info("Saving raw file '%s' (info and content) to Datastore",
                     name)
        rfc = RawFileContent(id=name, data=content, encoding=encoding)
        save_transactional([rfi, rfc])
    else:
        logging.info("Saving raw file '%s' (info only) to Datastore", name)
        rfi.put()


def save_tags_json(h2h):
    tags = h2h.sorted_tag_href_pairs()
    logging.info("Saving %d tag, href pairs", len(tags))
    TagsInfo(id="tags", tags=tags).put()


def to_html(name, content, encoding, h2h):
    content_str = None
    if encoding is None:
        try:
            encoding = 'UTF-8'
            content_str = content.decode(encoding)
        except UnicodeError:
            encoding = 'ISO-8859-1'
    if content_str is None:
        content_str = content.decode(encoding)
    html = h2h.to_html(name, content_str, encoding).encode()
    etag = base64.b64encode(sha1(html))
    datalen = len(html)
    phead = ProcessedFileHead(id=name, encoding=encoding.encode(), etag=etag)
    pparts = []
    if datalen > PFD_MAX_PART_LEN:
        phead.numparts = 0
        for i in range(0, datalen, PFD_MAX_PART_LEN):
            part = html[i:(i+PFD_MAX_PART_LEN)]
            if i == 0:
                phead.data0 = part
            else:
                partname = name + ':' + str(phead.numparts)
                pparts.append(ProcessedFilePart(id=partname, data=part,
                                                etag=etag))
            phead.numparts += 1
    else:
        phead.numparts = 1
        phead.data0 = html
    return phead, pparts, encoding


def urlfetch(client_pool, url, etag, headers=None):
    if headers is None:
        headers = {}
    if etag is not None:
        headers[HTTP_HDR_IF_NONE_MATCH] = etag.decode()
    logging.info("Fetching %s with headers %s", url, headers)
    url = geventhttpclient.URL(url)
    try:
        result = client_pool.get_client(url).get(url.request_uri, headers)
    except Exception as e:
        logging.error(e)
        raise UrlfetchError(e, url)
    logging.info("Fetched %s -> HTTP %s", url, result.status_code)
    return UrlfetchResponse(result)


class UrlfetchResponse:
    def __init__(self, ghc_resp):
        self.body = bytes(ghc_resp.read())
        ghc_resp.release()
        self._resp = ghc_resp

    @property
    def status_code(self):
        return self._resp.status_code

    def header(self, name):
        return self._resp.get(name)


class UrlfetchError(RuntimeError):
    def __init__(self, e, url):
        self._e = e
        self._url = url

    def __str__(self):
        return f"Failed to fetch {self._url}: {self._e}"


def handle_enqueue_update():
    req = flask.request

    is_cron = req.headers.get('X-AppEngine-Cron') == 'true'

    # https://cloud.google.com/appengine/docs/standard/python3/scheduling-jobs-with-cron-yaml?hl=en_GB#validating_cron_requests
    if not is_cron and os.environ.get('VIMHELP_ENV') != 'dev' and \
            secret.UPDATE_PASSWORD not in req.query_string:
        raise werkzeug.exceptions.Forbidden()

    logging.info("Enqueueing update")

    client = google.cloud.tasks.CloudTasksClient()
    queue_name = client.queue_path(os.environ['GOOGLE_CLOUD_PROJECT'],
                                   "us-central1",
                                   "update2")
    task = {
        'app_engine_http_request': {
            'http_method': 'POST',
            'relative_uri': '/update',
            'body': req.query_string
        }
    }
    response = client.create_task(parent=queue_name, task=task)
    logging.info('Task %s enqueued, ETA %s', response.name,
                 response.schedule_time)

    if is_cron:
        return flask.Response()
    else:
        return "Successfully enqueued update task."
